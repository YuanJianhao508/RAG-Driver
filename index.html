<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="RAG-Driver">
  <meta property="og:title" content="RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context
  Learning in Multi-Modal Large Language Model"/>
  <meta property="og:description" content="RAG-Driver is a Multi-Modal Large Language Model with Retrieval-augmented In-context Learning capacity designed for generalisable and explainable end-to-end driving with strong zeroshot generalisation capacity."/>
  <meta property="og:url" content="https://yuanjianhao508.github.io/RAG-Driver/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/RAGDriver_Teaser.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="RAG-Driver">
  <meta name="twitter:description" content="RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context
  Learning in Multi-Modal Large Language Model">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/RAGDriver_Teaser.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="RAG-Driver, Autonomous driving, Multi-modal Language
  Model, End-to-end Driving, In-Context Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context
    Learning in Multi-Modal Large Language Model</title>
  <link rel="icon" type="image/x-icon" href="static/images/car_13260.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" style="font-size: 1.8em;">RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context
              Learning in Multi-Modal Large Language Model</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://yuanjianhao508.github.io/" target="_blank">Jianhao Yuan</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://kevin-ssy.github.io/" target="_blank">Shuyang Sun</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://danielomeiza.github.io/" target="_blank">Daniel Omeiza</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.bozhao.me/" target="_blank">Bo Zhao</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://ori.ox.ac.uk/people/paul-newman/" target="_blank">Paul Newman</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://eng.ox.ac.uk/people/lars-kunze/" target="_blank">Lars Kunze</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://mttgdd.github.io/" target="_blank">Matthew Gadd</a><sup>1</sup>
              </span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>University of Oxford</span>
                    <span class="author-block"><sup>2</sup>Beijing Academy of Artificial Intelligence</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YuanJianhao508/RAG-Driver" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/demo.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        RAG-Driver is a Multi-Modal Large Language Model with Retrieval-augmented In-context Learning capacity designed for generalisable and explainable end-to-end driving with strong zeroshot generalisation capacity.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>  
          We need to trust robots that use often opaque AI methods. They need to explain themselves to us, and we need to trust their explanation. In this regard, explainability plays a critical role in trustworthy autonomous decision-making to foster transparency and acceptance among end users, especially in complex autonomous driving. Recent advancements in Multi-Modal Large Language models (MLLMs) have shown promising potential in enhancing the explainability as a driving agent producing control predictions along with natural language explanations. However, severe data scarcity due to expensive annotation costs and significant domain gaps between different datasets makes the development of a robust and generalisable system an extremely challenging task. Moreover, the prohibitively expensive training requirements of MLLM and the unsolved problem of catastrophic forgetting further limit their generalisability post-deployment.To address these challenges, we present RAG-Driver, a novel retrieval-augmented multi-modal large language model that leverages in-context learning for high-performance, explainable, and generalisable autonomous driving. By grounding in retrieved expert demonstration, we empirically validate that RAG-Driver achieves state-of-the-art performance in producing driving action explanations, justifications, and control signal prediction. More importantly, it exhibits exceptional zero-shot generalisation capabilities to unseen environments without further training endeavours.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/RAGDriver_Teaser.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          <strong>Right:</strong> In natural language, our system describes and justifies actions taken by the vehicle, as well as inferring the
          driving action in the form of numerical control signals (speed, and steering angle). For this, we use a unified perception and
          planning module through a Multi-modal Large Language Model. Our core contribution is a retrieval mechanism to search
          driving scenarios similar to the current condition and use these to augment the current predictions through In-Context Learning.
          This leads to better overall description and prediction and is more generalisable in new deployment domains. <strong>Left:</strong> Specialist
          and generalist baselines in in-distribution and out-of-distribution generalisation settings (horizontal axis).
          Our methods achieve better performance compared with all of the baselines, with a significant margin driving action explanation
          and justification tasks under CIDEr measure.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/RAGDriver_main.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          <strong>RAG-Driver Overview:</strong> Given a query comprising a video of the current driving scenario and its corresponding control
          signal, the process starts with the input being fed into a Retrieval Engine. It searches within a memory database for driving
          experiences that are similar to the current scenario, thereby providing relevant in-context learning samples. Subsequently, a
          Multi-Modal Large Language Model (MLLM) processes both the current query and the retrieved in-context learning samples.
          Based on high-level task instructions, the model engages in various prediction tasks: Action Explanation, Action Justification,
          and Next Control Signal Prediction.
        </h2>
      </div>
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
